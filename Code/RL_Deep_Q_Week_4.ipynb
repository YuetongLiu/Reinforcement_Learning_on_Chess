{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyvirtualdisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "import PIL.Image\n",
    "from tf_agents.policies import random_tf_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "#display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluation(Testing)\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "# Python to TF\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent / Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y(s,a,r,s‚Ä≤)=r+Œ≥maxa‚Ä≤QŒ∏‚àí(s‚Ä≤,a‚Ä≤)\n",
    "\n",
    "L(Œ∏)=ùîº(s,a,r,s‚Ä≤)‚àºU(D)[(Y(s,a,r,s‚Ä≤)‚àíQŒ∏(s,a))2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a QNetwork\n",
    "Feed Forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,) # initial_collect_steps? input layer?\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    # A list of fully_connected parameters, \n",
    "    # where each item is the number of units in the layer\n",
    "    fc_layer_params=fc_layer_params) \n",
    "#q_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a DqnAgent \n",
    "\n",
    "Implements the DQN algorithm from\n",
    "\n",
    "\"Human level control through deep reinforcement learning\" Mnih et al., 2015\n",
    "\n",
    "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate) # Adam algorithm\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer, # AdamOptimizer function\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss, # loss function\n",
    "    train_step_counter=train_step_counter) # integer step counter\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies/Rules\n",
    "\n",
    "The rules will return an action to produce the desired rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy # The main policy that is used for evaluation and deployment.\n",
    "collect_policy = agent.collect_policy # A second policy that is used for data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Computes the average return of a policy per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show baseline performance by randomly selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "Dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "       dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "       dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))),\n",
       " ('step_type',\n",
       "  'observation',\n",
       "  'action',\n",
       "  'policy_info',\n",
       "  'next_step_type',\n",
       "  'reward',\n",
       "  'discount'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    # The maximum number of items that can be stored in a \n",
    "    # single batch segment of the buffer\n",
    "    max_length=replay_buffer_max_length) \n",
    "\n",
    "agent.collect_data_spec, agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert the dictionary as a data set and return an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "#dataset\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "collect data from the environment\n",
    "\n",
    "use that data to train the agent's neural network(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 18.008861541748047\n",
      "step = 400: loss = 5.086471080780029\n",
      "step = 600: loss = 15.28239631652832\n",
      "step = 800: loss = 9.468496322631836\n",
      "step = 1000: loss = 11.550569534301758\n",
      "step = 1000: Average Return = 15.699999809265137\n",
      "step = 1200: loss = 35.28739929199219\n",
      "step = 1400: loss = 2.0952863693237305\n",
      "step = 1600: loss = 2.0557005405426025\n",
      "step = 1800: loss = 25.886310577392578\n",
      "step = 2000: loss = 5.963220119476318\n",
      "step = 2000: Average Return = 24.299999237060547\n",
      "step = 2200: loss = 11.770583152770996\n",
      "step = 2400: loss = 8.902961730957031\n",
      "step = 2600: loss = 37.822486877441406\n",
      "step = 2800: loss = 6.025323867797852\n",
      "step = 3000: loss = 9.689393043518066\n",
      "step = 3000: Average Return = 110.4000015258789\n",
      "step = 3200: loss = 22.36556625366211\n",
      "step = 3400: loss = 32.306640625\n",
      "step = 3600: loss = 15.72234058380127\n",
      "step = 3800: loss = 6.93696403503418\n",
      "step = 4000: loss = 50.098915100097656\n",
      "step = 4000: Average Return = 90.0\n",
      "step = 4200: loss = 47.52417755126953\n",
      "step = 4400: loss = 155.73275756835938\n",
      "step = 4600: loss = 153.7744140625\n",
      "step = 4800: loss = 112.13093566894531\n",
      "step = 5000: loss = 15.244081497192383\n",
      "step = 5000: Average Return = 114.5999984741211\n",
      "step = 5200: loss = 21.695892333984375\n",
      "step = 5400: loss = 5.562005996704102\n",
      "step = 5600: loss = 6.435539245605469\n",
      "step = 5800: loss = 59.24911117553711\n",
      "step = 6000: loss = 7.506043910980225\n",
      "step = 6000: Average Return = 160.0\n",
      "step = 6200: loss = 65.04222869873047\n",
      "step = 6400: loss = 217.68240356445312\n",
      "step = 6600: loss = 174.95286560058594\n",
      "step = 6800: loss = 164.0635528564453\n",
      "step = 7000: loss = 10.535503387451172\n",
      "step = 7000: Average Return = 114.69999694824219\n",
      "step = 7200: loss = 117.93932342529297\n",
      "step = 7400: loss = 67.07676696777344\n",
      "step = 7600: loss = 10.39604377746582\n",
      "step = 7800: loss = 67.12391662597656\n",
      "step = 8000: loss = 105.36053466796875\n",
      "step = 8000: Average Return = 198.5\n",
      "step = 8200: loss = 118.29344177246094\n",
      "step = 8400: loss = 140.25460815429688\n",
      "step = 8600: loss = 30.344758987426758\n",
      "step = 8800: loss = 11.699833869934082\n",
      "step = 9000: loss = 476.17034912109375\n",
      "step = 9000: Average Return = 176.1999969482422\n",
      "step = 9200: loss = 451.31793212890625\n",
      "step = 9400: loss = 20.707763671875\n",
      "step = 9600: loss = 28.288009643554688\n",
      "step = 9800: loss = 125.00628662109375\n",
      "step = 10000: loss = 165.96885681152344\n",
      "step = 10000: Average Return = 164.39999389648438\n",
      "step = 10200: loss = 234.79196166992188\n",
      "step = 10400: loss = 34.319217681884766\n",
      "step = 10600: loss = 73.28331756591797\n",
      "step = 10800: loss = 21.278406143188477\n",
      "step = 11000: loss = 207.4669189453125\n",
      "step = 11000: Average Return = 196.5\n",
      "step = 11200: loss = 9.321070671081543\n",
      "step = 11400: loss = 34.81235885620117\n",
      "step = 11600: loss = 24.20108985900879\n",
      "step = 11800: loss = 218.18382263183594\n",
      "step = 12000: loss = 54.903507232666016\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 696.0888061523438\n",
      "step = 12400: loss = 118.06753540039062\n",
      "step = 12600: loss = 29.727664947509766\n",
      "step = 12800: loss = 172.8981475830078\n",
      "step = 13000: loss = 36.71021270751953\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 43.96297836303711\n",
      "step = 13400: loss = 187.39060974121094\n",
      "step = 13600: loss = 699.5609741210938\n",
      "step = 13800: loss = 43.06932830810547\n",
      "step = 14000: loss = 262.7250061035156\n",
      "step = 14000: Average Return = 199.5\n",
      "step = 14200: loss = 44.86981201171875\n",
      "step = 14400: loss = 788.365966796875\n",
      "step = 14600: loss = 186.86534118652344\n",
      "step = 14800: loss = 34.16826248168945\n",
      "step = 15000: loss = 68.5713882446289\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 36.6361083984375\n",
      "step = 15400: loss = 67.6352767944336\n",
      "step = 15600: loss = 88.42098999023438\n",
      "step = 15800: loss = 66.49943542480469\n",
      "step = 16000: loss = 34.3782958984375\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 51.016273498535156\n",
      "step = 16400: loss = 979.0067138671875\n",
      "step = 16600: loss = 20.1558780670166\n",
      "step = 16800: loss = 74.05459594726562\n",
      "step = 17000: loss = 27.95159912109375\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 149.70065307617188\n",
      "step = 17400: loss = 16.747379302978516\n",
      "step = 17600: loss = 678.1433715820312\n",
      "step = 17800: loss = 41.89617919921875\n",
      "step = 18000: loss = 609.5775756835938\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 454.809326171875\n",
      "step = 18400: loss = 48.18207550048828\n",
      "step = 18600: loss = 48.29536437988281\n",
      "step = 18800: loss = 122.12271118164062\n",
      "step = 19000: loss = 1473.701416015625\n",
      "step = 19000: Average Return = 200.0\n",
      "step = 19200: loss = 135.0766143798828\n",
      "step = 19400: loss = 408.56280517578125\n",
      "step = 19600: loss = 489.67938232421875\n",
      "step = 19800: loss = 107.77105712890625\n",
      "step = 20000: loss = 753.5126953125\n",
      "step = 20000: Average Return = 200.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.484999799728394, 250.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
